# Generalized VAE with PixelCNN Decoder

This repo implements the methods described in (Cite paper). A VAE with powerful decoding family such as PixelCNN tend to ignore latent code (Cite paper), and use only the decoding distribution to represent the entire dataset. (Cite paper) showed that this phenomenon is not general. For a more general family of VAE models, there are members that prefer to use the latent code. In particular, without any regularization on the posterior the model will prefer to use latent code. Furthermore we can still obtain correct samples, albeit only through a Markov chain.

- Samples generated by model without regularization

![mc_noreg](plots/pixel_vae_cifar_mc_noreg.png)

- Samples generated by model with ELBO regularization

![mc_elbo](plots/pixel_vae_cifar_mc_elbo.png)

This is a fork of \citep{Salimans2016PixeCNN} [PixelCNN++](https://openreview.net/pdf?id=BJrFC6ceg). The decoding distribution is unchanged. The difference is the addition of encoder and regularization, and visualizations.



```
@inproceedings{Salimans2016PixeCNN,
  title={PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications},
  author={Tim Salimans and Andrej Karpathy and Xi Chen and Diederik P. Kingma and Yaroslav Bulatov},
  booktitle={Submitted to ICLR 2017},
  year={2016}
}
```
